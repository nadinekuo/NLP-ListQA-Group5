{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "!pip install transformers\n",
    "!pip install bert-score\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import BERTScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_time_ranges(entity_list):\n",
    "    time_ranges = []\n",
    "    for entity in entity_list:\n",
    "        # Extract the time ranges using a regular expression\n",
    "        match = re.search(r'\\((\\d{4}.*)\\)', entity)\n",
    "        if match:\n",
    "            time_range = f\"({match.group(1)})\"\n",
    "            time_ranges.append(time_range)\n",
    "    return time_ranges\n",
    "\n",
    "def extract_entities(entity_list):\n",
    "    entities = []\n",
    "    for entity in entity_list:\n",
    "        # Extract the part before the parentheses or part within parentheses that does not contain digits\n",
    "        match = re.match(r'(.*?)(?: \\((\\D+?)\\))? \\((\\d{4}.*)\\)', entity)\n",
    "        if match:\n",
    "            entity_name = match.group(1).strip()\n",
    "            entities.append(entity_name)\n",
    "        else:\n",
    "            entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def compute_time_bleu(gt_list, pred_list, tokenizer):\n",
    "    gt_time_list = extract_time_ranges(gt_list)\n",
    "    pred_time_list = extract_time_ranges(pred_list)\n",
    "\n",
    "    if len(pred_time_list) == 0:\n",
    "        return 0\n",
    "\n",
    "    gt_time_list = list(map(lambda x: tokenizer(x, return_tensors=\"pt\").input_ids[0].tolist(), gt_time_list))\n",
    "    pred_time_list = list(map(lambda x: tokenizer(x, return_tensors=\"pt\").input_ids[0].tolist(), pred_time_list))\n",
    "\n",
    "    gt_time_list = [lst[:-1] if lst and lst[-1] == 1 else lst for lst in gt_time_list]\n",
    "    pred_time_list = [lst[:-1] if lst and lst[-1] == 1 else lst for lst in pred_time_list]\n",
    "\n",
    "    total_bleu = 0\n",
    "    for i, pred in enumerate(pred_time_list):\n",
    "        curr_bleu = sentence_bleu(gt_time_list, pred)\n",
    "        total_bleu += curr_bleu\n",
    "    total_bleu = total_bleu / len(pred_time_list)\n",
    "    return total_bleu\n",
    "\n",
    "def compute_em(gt_list, pred_list):\n",
    "    # Calculate the match score for each ground truth item\n",
    "    matches = [1 if gt in pred_list else 0 for gt in gt_list]\n",
    "\n",
    "    # Compute the average exact match score based on the length of the ground truth list\n",
    "    exact_match_score = sum(matches) / len(gt_list)\n",
    "\n",
    "    return exact_match_score\n",
    "\n",
    "def compute_f1(gt_tokens, pred_tokens):\n",
    "    precision = compute_precision(gt_tokens, pred_tokens)\n",
    "    recall = compute_recall(gt_tokens, pred_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "# Computed as # correct tokens / # pred tokens\n",
    "def compute_precision(gt_tokens, pred_tokens):\n",
    "    prec = []\n",
    "    for gt, pred in zip(gt_tokens, pred_tokens):\n",
    "        true_positives = len(set(gt) & set(pred))\n",
    "        if len(pred) == 0:\n",
    "            prec.append(0.0)\n",
    "        prec.append(true_positives / len(pred))\n",
    "    return np.mean(prec)\n",
    "\n",
    "\n",
    "# Evaluates completeness: # correct answers / # GT answers\n",
    "def compute_recall(gt_tokens, pred_tokens):\n",
    "    rec = []\n",
    "    for gt, pred in zip(gt_tokens, pred_tokens):\n",
    "        true_positives = len(set(gt) & set(pred))\n",
    "        if len(gt) == 0:\n",
    "            rec.append(0.0)\n",
    "        rec.append(true_positives / len(gt))\n",
    "    return np.mean(rec)\n",
    "\n",
    "def compute_entity_bert(gt_list, pred_list):\n",
    "    gt_entity_list = extract_entities(gt_list)\n",
    "    pred_entity_list = extract_entities(pred_list)\n",
    "\n",
    "    if len(pred_entity_list) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Concatenate all strings in pred and gt lists\n",
    "    concat_gt = ' '.join(gt_entity_list)\n",
    "    concat_pred = ' '.join(pred_entity_list)\n",
    "\n",
    "    scorer = BERTScorer(model_type='bert-base-uncased')  # NOTE: this can take some time to load\n",
    "    P, R, F1 = scorer.score([concat_gt], [concat_pred])\n",
    "    # print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
    "    return F1.tolist()[0]    # Convert Tensor to List\n",
    "\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    bleu_score = compute_time_bleu(decoded_labels, decoded_preds, tokenizer)\n",
    "    em = compute_em(decoded_labels, decoded_preds)\n",
    "    f1 = compute_f1(labels, predictions)\n",
    "    recall = compute_recall(labels, predictions)\n",
    "    entity_bert = compute_entity_bert(decoded_labels, decoded_preds)\n",
    "\n",
    "    return {'em': em, 'f1': f1, 'recall': recall, 'bleu': bleu_score, 'Entity BERT': entity_bert}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartForQuestionAnswering, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open('/kaggle/input/tlqa2dataset/train_TLQA.json', 'r') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open('/kaggle/input/tlqa2dataset/val_TLQA.json', 'r') as f:\n",
    "    val = json.load(f)\n",
    "\n",
    "# Flatten the answers\n",
    "def flatten_answers(answers):\n",
    "    return ', '.join(answers) if isinstance(answers, list) else answers\n",
    "\n",
    "train_dict = {\n",
    "    \"question\": [item[\"question\"] for item in train],\n",
    "    \"answers\": [flatten_answers(item['answers']) for item in train]\n",
    "}\n",
    "\n",
    "val_dict = {\n",
    "    \"question\": [item[\"question\"] for item in val],\n",
    "    \"answers\": [flatten_answers(item['answers']) for item in val]\n",
    "}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "val_dataset = Dataset.from_dict(val_dict)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Tokenize the dataset\n",
    "# Tokenize the dataset\n",
    "def preprocess_data(examples):\n",
    "    inputs = examples['question']\n",
    "    targets = examples['answers']\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    # Replace padding token id's in the labels by -100, so they're ignored in the loss computation\n",
    "    model_inputs['labels'] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels_example]\n",
    "        for labels_example in model_inputs['labels']\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',          # Directory to save model output and checkpoints\n",
    "    num_train_epochs=2,              # Number of epochs to train the model\n",
    "    per_device_train_batch_size=4,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Weight decay for regularization\n",
    "    logging_dir='./logs',            # Directory to save logs\n",
    "    logging_steps=10,                # Log metrics every specified number of steps\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "model.save_pretrained('./my_model_v2', safe_serialization=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
