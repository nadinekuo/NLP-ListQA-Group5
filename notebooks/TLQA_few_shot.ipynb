{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot TLQA with FlanT5\n",
    "\n",
    "Below, we run generative models FlanT5-large and FlanT5-XL in a few-shot setting by selecting demonstration examples from training set of TLQA, which was derived from tempLLama.\n",
    "- `flan-t5-large`: 780M params, 1GB mem\n",
    "- `flan-t5-xl`: 3B params, 12 GB mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:53:28.743958Z",
     "start_time": "2024-06-08T07:53:21.951515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: sentence-transformers in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (3.0.0)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (4.37.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (2.3.0)\r\n",
      "Requirement already satisfied: numpy in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.5.0)\r\n",
      "Requirement already satisfied: scipy in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (0.23.2)\r\n",
      "Requirement already satisfied: Pillow in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.1)\r\n",
      "Requirement already satisfied: sympy in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: langchain-core in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (0.2.3)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langchain-core) (6.0.1)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langchain-core) (1.33)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.65 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langchain-core) (0.1.67)\r\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langchain-core) (23.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langchain-core) (2.7.2)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langchain-core) (8.3.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.65->langchain-core) (3.10.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.65->langchain-core) (2.32.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core) (2.18.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core) (4.12.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (2024.6.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: bert-score in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (0.3.13)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (2.3.0)\r\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (2.2.2)\r\n",
      "Requirement already satisfied: transformers>=3.0.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (4.37.2)\r\n",
      "Requirement already satisfied: numpy in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (4.66.4)\r\n",
      "Requirement already satisfied: matplotlib in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (3.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from bert-score) (23.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\r\n",
      "Requirement already satisfied: filelock in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.14.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (4.12.1)\r\n",
      "Requirement already satisfied: sympy in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (2024.3.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.4.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from matplotlib->bert-score) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from matplotlib->bert-score) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from matplotlib->bert-score) (4.53.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from matplotlib->bert-score) (1.4.5)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from matplotlib->bert-score) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from matplotlib->bert-score) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->bert-score) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->bert-score) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->bert-score) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from requests->bert-score) (2024.6.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.37.2 optimum==1.12.0 --quiet\n",
    "# !pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --quiet\n",
    "# !pip install langchain==0.1.9 --quiet\n",
    "# # !pip install chromadb\n",
    "# !pip install sentence_transformers==2.4.0 --quiet\n",
    "# !pip install unstructured --quiet\n",
    "# !pip install pdf2image --quiet\n",
    "# !pip install pdfminer.six==20221105 --quiet\n",
    "# !pip install unstructured-inference --quiet\n",
    "# !pip install faiss-gpu==1.7.2 --quiet\n",
    "# !pip install pikepdf==8.13.0 --quiet\n",
    "# !pip install pypdf==4.0.2 --quiet\n",
    "# !pip install pillow_heif==0.15.0 --quiet\n",
    "\n",
    "!pip install transformers==4.37.2 optimum==1.12.0 --quiet\n",
    "!pip install sentence-transformers\n",
    "!pip install langchain-core\n",
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running FlanT5-Large and FlanT5-XL (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:53:47.754005Z",
     "start_time": "2024-06-08T07:53:40.618042Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m T5Tokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle/flan-t5-large\u001B[39m\u001B[38;5;124m\"\u001B[39m, torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16)\n\u001B[0;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mT5ForConditionalGeneration\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgoogle/flan-t5-large\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\u001B[39;00m\n\u001B[1;32m     11\u001B[0m input_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mList all Michael Jackson albums between 2000 and 2009.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2595\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2590\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[1;32m   2591\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2592\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2593\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2594\u001B[0m         )\n\u001B[0;32m-> 2595\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1173\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1170\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1171\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m-> 1173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    777\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    778\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 779\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    789\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    790\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:804\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    800\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    801\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    803\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 804\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    805\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    807\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1159\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1152\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1153\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1154\u001B[0m             device,\n\u001B[1;32m   1155\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1156\u001B[0m             non_blocking,\n\u001B[1;32m   1157\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[1;32m   1158\u001B[0m         )\n\u001B[0;32m-> 1159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    288\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to('cuda')\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "input_text = \"List all Michael Jackson albums between 2000 and 2009.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:54:45.733987Z",
     "start_time": "2024-06-08T07:54:45.707258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212 \n",
      "\n",
      "question: List all entities that owned Absolute Radio, also known as Virgin 1215, from 2010 to 2020.\n",
      "answers: ['The Times Group (2010, 2011, 2012, 2013)', 'Bauer Radio (2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "type: P127\n",
      "subject: Absolute Radio\n",
      "answer_ids: {'The Times Group': 'Q972645', 'Bauer Radio': 'Q4873460'}\n",
      "wikidata_ID: Q4590187\n",
      "verified: True\n",
      "subject_label: Absolute Radio\n",
      "aliases: ['Virgin 1215', 'Virgin Radio', 'Virgin 105.8']\n",
      "final_answers: ['The Times Group (2010, 2011, 2012, 2013)', 'Bauer Radio (2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "import src.utils as utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "train_set = utils.json_to_list(\"../data/train_TLQA.json\")\n",
    "test_set = utils.json_to_list(\"../data/test_TLQA.json\")\n",
    "print(len(train_set), '\\n')\n",
    "example = train_set[45]\n",
    "for key in example.keys():\n",
    "    print(f\"{key}: {example[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Example Generation using KNN Search\n",
    "\n",
    "To select few-shot examples, we select examples from the training set that are close to each test example by using KNN search in a continuous vector space using any embedding model from sentence-transformers.\n",
    "We reuse the KNN search from https://anonymous.4open.science/r/ICAT-47FC/code/2wikimultihopqa/get_chatgpt_skill_transfer_knn.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:54:51.581811Z",
     "start_time": "2024-06-08T07:54:49.965759Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class KnnSearch:\n",
    "    def __init__(self, data=None, num_trees=None, emb_dim=None):\n",
    "        self.num_trees = num_trees\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def get_embeddings_for_data(self, data_ls):\n",
    "        # NOTE: any embedding model from sentence-transformers can be used\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        embeddings = model.encode(data_ls)\n",
    "        return embeddings\n",
    "\n",
    "    def get_top_n_neighbours(self, sentence, data_emb, transfer_data, k):\n",
    "        \"\"\"\n",
    "        Retrieves the top k most similar questions for \"sentence\" based on cosine similarity from given embeddings \"data_emb\".\n",
    "\n",
    "        Parameters:\n",
    "        sentence (str): The input sentence to find similar questions for.\n",
    "        data_emb (np.ndarray): The embeddings for the transfer questions.\n",
    "        transfer_data (list): The list of transfer questions corresponding to data_emb.\n",
    "        k (int): The number of top similar questions to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of the top k similar questions from transfer_data and all similar questions from str_qa.\n",
    "        \"\"\"\n",
    "        sent_emb = self.get_embeddings_for_data(sentence)\n",
    "        top_questions = []\n",
    "\n",
    "        text_sims = cosine_similarity(data_emb,[sent_emb]).tolist()\n",
    "        results_sims = zip(range(len(text_sims)), text_sims)\n",
    "        sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)  # Obtain the highest similarities\n",
    "\n",
    "        # NOTE: we only match based on questions, but include the full question-answer pair in resulting neighs\n",
    "        for idx, item in sorted_similarities[:k]:\n",
    "            top_questions.append(transfer_data[idx])\n",
    "\n",
    "        return top_questions\n",
    "    \n",
    "    def semantic_score(self, query_embedding, document_embeddings):\n",
    "        return np.dot(query_embedding, document_embeddings)\n",
    "\n",
    "    def semantic_values(self, questions: list):\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        query_embedding = model.encode(questions)\n",
    "\n",
    "        semantic_scores = []\n",
    "        for question_emb in query_embedding:\n",
    "            for other_question_emb in query_embedding:\n",
    "                if not np.array_equal(other_question_emb, question_emb):\n",
    "                    semantic_scores.append(self.semantic_score(question_emb, other_question_emb))\n",
    "\n",
    "        return np.mean(np.array(semantic_scores)), np.std(np.array(semantic_scores))\n",
    "\n",
    "    def temporal_score(self, sentence_timestamp: datetime, data_emb_timestamp: datetime):\n",
    "        alpha = 1.0\n",
    "        return alpha / (data_emb_timestamp.timestamp() - sentence_timestamp.timestamp())\n",
    "\n",
    "    def get_top_n_temp_neighbours(self, sentence_timestamp: datetime, data_emb_timestamps: list[datetime],\n",
    "                                  standard_deviation_sematic: float, mean_sematic: float, transfer_data, k: int):\n",
    "        \"\"\"\n",
    "        Retrieves the top k most similar questions for \"sentence\" based on cosine similarity from given embeddings \"data_emb\".\n",
    "\n",
    "        Parameters:\n",
    "        sentence_timestamp (datetime): The timestamp of the sentence.\n",
    "        sentence (str): The input sentence to find similar questions for.\n",
    "        data_emb_timestamps (np.ndarray): The timestamps of all data questions.\n",
    "        transfer_data (list): The list of transfer questions corresponding to data_emb.\n",
    "        k (int): The number of top similar questions to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of the top k similar questions from transfer_data and all similar questions from str_qa.\n",
    "        \"\"\"\n",
    "        timestamps = np.array([dt.timestamp() for dt in data_emb_timestamps])\n",
    "        mean_timestamp = np.mean(timestamps)\n",
    "        standard_deviation_timestamp = np.std(timestamps)\n",
    "\n",
    "        temp_scores = []\n",
    "        for data_emb_timestamp in data_emb_timestamps:\n",
    "            temp_score = ((self.temporal_score(sentence_timestamp,\n",
    "                                               data_emb_timestamp) - mean_timestamp) / standard_deviation_timestamp) * standard_deviation_sematic + mean_sematic\n",
    "            temp_scores.append(temp_score)\n",
    "\n",
    "        results_sims = zip(range(len(temp_scores)), temp_scores)\n",
    "        sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_questions = []\n",
    "        for idx, item in sorted_similarities[:k]:\n",
    "            top_questions.append(transfer_data[idx])\n",
    "\n",
    "        return top_questions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:54:55.423071Z",
     "start_time": "2024-06-08T07:54:55.417100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracts all questions from the (train) set used for getting neighbours\n",
    "def get_transfer_questions(transfer_data):\n",
    "    transfer_questions = []\n",
    "    for index, data in enumerate(transfer_data):\n",
    "        transfer_questions.append(data[\"question\"])\n",
    "    return transfer_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:55:06.211023Z",
     "start_time": "2024-06-08T07:54:56.172679Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "knn = KnnSearch()\n",
    "\n",
    "# Read raw dataset to a list containing question and answers (no metadata)\n",
    "train_data = utils.json_to_list(\"../data/train_TLQA.json\")\n",
    "# Keep questions only to embed (to use in similarity metric)\n",
    "train_questions = get_transfer_questions(train_data)\n",
    "train_questions_emb = knn.get_embeddings_for_data(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amereuta/Desktop/University/NLP/NLP-ListQA-Group5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all positions held by Mark Zuckerberg from 2000 to 2024.\n",
      "List all sports teams Zvjezdan Misimović, also known as Zvjezdan Misimovic, played for from 2010 to 2014.\n",
      "['Galatasaray S.K. (2010, 2011)', 'VfL Wolfsburg (2010)', 'Bosnia and Herzegovina national football team (2010, 2011, 2012, 2013, 2014)', 'FC Dinamo Moscow (2011, 2012, 2013)', 'Guizhou Renhe F.C. (2013, 2014)']\n",
      "\n",
      "\n",
      "List all educational institutions Jazmin Grace Grimaldi, also known as Jazmin Grimaldi, attended from 2010 to 2014.\n",
      "['Fordham University (2010, 2011, 2012, 2013, 2014)', 'JSerra Catholic High School (2010)']\n",
      "\n",
      "\n",
      "List all heads of the government of Nagykanizsa, also known as Nagy-Kanizsa, from 2010 to 2014 \n",
      "['Péter Cseresnyés (2010, 2011, 2012, 2013, 2014)', 'István Marton (2010)']\n",
      "\n",
      "\n",
      "mean semantic score:  0.33486798\n",
      "std semantic score:  0.1371694\n"
     ]
    }
   ],
   "source": [
    "from src.metrics import *\n",
    "\n",
    "train_data = utils.json_to_list(\"../data/train_TLQA.json\")\n",
    "train_questions = get_transfer_questions(train_data)\n",
    "\n",
    "year_pattern = r'\\b\\d{4}\\b'\n",
    "def extract_years_and_convert_to_datetime(sentence):\n",
    "    years = re.findall(year_pattern, sentence)\n",
    "    timestamps = []\n",
    "    for year in years:\n",
    "        date_string = f\"January 1, {year}\"  # Assuming January 1 for simplicity\n",
    "        date_object = datetime.strptime(date_string, \"%B %d, %Y\")\n",
    "        timestamps.append(date_object.timestamp())\n",
    "    \n",
    "    return datetime.fromtimestamp(np.mean(np.array(timestamps)))\n",
    "\n",
    "mean_semantic_score, std_semantic_score = knn.semantic_values(train_questions)\n",
    "years = [extract_years_and_convert_to_datetime(train_question) for train_question in train_questions]\n",
    "\n",
    "question = \"List all positions held by Mark Zuckerberg from 2000 to 2024.\"\n",
    "sentence_timestamp = extract_years_and_convert_to_datetime(question) \n",
    "print(question)\n",
    "\n",
    "neighs = knn.get_top_n_temp_neighbours(sentence_timestamp, years, std_semantic_score, mean_semantic_score, train_data, k=3)\n",
    "for neigh in neighs:\n",
    "    print(neigh['question'])\n",
    "    print(neigh['answers'])\n",
    "    print('\\n')\n",
    "\n",
    "print(\"mean semantic score: \", mean_semantic_score)\n",
    "print(\"std semantic score: \", std_semantic_score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T07:55:41.525757Z",
     "start_time": "2024-06-08T07:55:13.632816Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all positions Per Sandberg held from 2015 to 2018.\n",
      "['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']\n",
      "\n",
      "\n",
      "List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n",
      "\n",
      "\n",
      "List all positions David M. O'Connell held from 2010 to 2020.\n",
      "['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "# TODO: loop over test samples during/before inference time to create few-shot prompts\n",
    "# question = \"List all employers Elon Musk worked for from 1990 to 2024.\"\n",
    "question = \"List all positions held by Mark Zuckerberg from 2000 to 2024.\"\n",
    "neighs = knn.get_top_n_neighbours(sentence=question, data_emb=train_questions_emb, transfer_data=train_data, k=3)   # data_emb is embedded questions only, so we only match questions\n",
    "for neigh in neighs:\n",
    "    print(neigh['question'])\n",
    "    print(neigh['answers'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Few-Shot Prompt Template using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'List all positions Per Sandberg held from 2015 to 2018.', 'answers': ['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']}, {'question': 'List all positions Mark Drakeford held from 2013 to 2020.', 'answers': ['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']}, {'question': \"List all positions David M. O'Connell held from 2010 to 2020.\", 'answers': ['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']}]\n"
     ]
    }
   ],
   "source": [
    "def simplify_dict_list(dict_list):\n",
    "    return [{'question': item['question'], 'answers': item['answers']} for item in dict_list]\n",
    "\n",
    "simple_neighs = simplify_dict_list(neighs)\n",
    "print(simple_neighs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we configure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answers\"], template=\"Question: {question}\\n{answers}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**simple_neighs[1]))\n",
    "# print(example_prompt.format(question=f\"{simple_neighs[0]['question']}\", answers=f\"{simple_neighs[0]['answers']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed examples and formatter to FewShotPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List all positions Per Sandberg held from 2015 to 2018.\n",
      "['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']\n",
      "\n",
      "Question: List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions David M. O'Connell held from 2010 to 2020.\n",
      "['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions held by Mark Zuckerberg between 2000 and 2020. Please answer this question in the same format as the three examples above.\n"
     ]
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=simple_neighs,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "few_shot_prompt = prompt.format(input=\"List all positions held by Mark Zuckerberg between 2000 and 2020. Please answer this question in the same format as the three examples above.\")\n",
    "print(few_shot_prompt)   # String to feed to model during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLQA Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "# TODO: uncomment when using GPU\n",
    "# import accelerate\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Uncomment the line below to run on GPU\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5/5 [00:00<00:00, 1595.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "K = 10   # TODO: Experiment with 3, 5, 7, 10\n",
    "MAX_OUTPUT_LEN = 200\n",
    "MODEL_NAME = \"flan-t5-large\"  # TODO: Set accordingly\n",
    "\n",
    "results_GT_dict = {'prompts': [], 'outputs': [], 'output_tokens': [], \n",
    "                            'ground_truths': [], 'ground_truth_tokens': []}  \n",
    "\n",
    "# Configure formatter that will format the few-shot examples into a string\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answers\"], template=\"Question: {question}\\n{answers}\"\n",
    ")\n",
    "\n",
    "# Convert test set to list and loop over all items (1071 in total)\n",
    "for i, item in enumerate(test_set):\n",
    "    \n",
    "    # For each test question, retrieve k neighbours (try 3, 5, 7, 10)\n",
    "    test_question = test_set[i]['question']\n",
    "    neighs = knn.get_top_n_neighbours(sentence=test_question, data_emb=train_questions_emb, transfer_data=train_data, k=K)\n",
    "    simple_neighs = simplify_dict_list(neighs)\n",
    "\n",
    "    # Create the few-shot prompt template and feed to model\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=simple_neighs,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"Question: {input}\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "    few_shot_prompt = prompt.format(input=f\"{test_question} Please answer this question in the same format as the {K} examples above.\")\n",
    "    results_GT_dict['prompts'].append(few_shot_prompt)\n",
    "    \n",
    "    # TODO: pre-tokenize input and GT to speed up inference\n",
    "    input_ids = tokenizer(few_shot_prompt, return_tensors=\"pt\").input_ids\n",
    "    # TODO: ---------------------- UNCOMMENT WHEN USING GPU -----------------------------\n",
    "    # input_ids = tokenizer(few_shot_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    output_tokens = model.generate(input_ids, max_length=MAX_OUTPUT_LEN)\n",
    "    output = tokenizer.decode(output_tokens[0])\n",
    "\n",
    "    results_GT_dict['output_tokens'].append(output_tokens[0])\n",
    "    results_GT_dict['outputs'].append(output)\n",
    "    results_GT_dict['ground_truths'].append(test_set[i]['final_answers'])\n",
    "    gt_tokens = tokenizer(str(test_set[i]['final_answers']), return_tensors=\"pt\").input_ids[0]\n",
    "    results_GT_dict['ground_truth_tokens'].append(gt_tokens)\n",
    "\n",
    "results_ds = Dataset.from_dict(results_GT_dict)\n",
    "results_ds.save_to_disk(f\"{K}_shot_{MODEL_NAME}.hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the results Dataset from disk and inspect individual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompts', 'outputs', 'output_tokens', 'ground_truths', 'ground_truth_tokens'],\n",
      "    num_rows: 5\n",
      "})\n",
      "\n",
      "MODEL OUTPUT:\n",
      "<pad> ['Teachta Dála (2010, 2012, 2013, 2014)', 'Labour Party (2010, 2011, 2012, 2013, 2014)', 'Labour Party (2010, 2011, 2012, 2013, 2014)', 'Labour Party (2010, 2011, 2012, 2013, 2014)']</s>\n",
      "\n",
      "OUTPUT TOKENS:\n",
      "[0, 784, 31, 382, 15, 9, 3997, 9, 309, 2975, 521, 41, 14926, 6, 1673, 6, 7218, 1412, 61, 31, 6, 3, 31, 18506, 1211, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 61, 31, 6, 3, 31, 18506, 1211, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 61, 31, 6, 3, 31, 18506, 1211, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 61, 31, 908, 1]\n",
      "\n",
      "GROUND TRUTH:\n",
      "['Socialist Party (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)', 'RISE (Ireland) (2019, 2020)']\n",
      "\n",
      "GT TOKENS:\n",
      "[784, 31, 5231, 4703, 343, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 6, 1230, 6, 5123, 4791, 4323, 1360, 61, 31, 6, 3, 31, 13431, 427, 41, 196, 60, 40, 232, 61, 41, 8584, 6, 6503, 61, 31, 908, 1]\n"
     ]
    }
   ],
   "source": [
    "results_ds = Dataset.load_from_disk(\"../results/10_shot_flan-t5-large_test.hf\")\n",
    "print(results_ds)\n",
    "idx_to_inspect = 2\n",
    "\n",
    "prompt = results_ds['prompts'][idx_to_inspect]\n",
    "output = results_ds['outputs'][idx_to_inspect]\n",
    "output_tokens = results_ds['output_tokens'][idx_to_inspect]\n",
    "gt_tokens = results_ds['ground_truth_tokens'][idx_to_inspect]\n",
    "gt = results_ds['ground_truths'][idx_to_inspect]\n",
    "\n",
    "# print(f\"\\nPROMPT (idx = {idx_to_inspect}):\\n{example_prompt}\\n\")\n",
    "print(f\"\\nMODEL OUTPUT:\\n{output}\\n\\nOUTPUT TOKENS:\\n{output_tokens}\")\n",
    "print(f\"\\nGROUND TRUTH:\\n{gt}\\n\\nGT TOKENS:\\n{gt_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will test the utility functions that compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred time ranges: ['(2010, 2012, 2013, 2014)', '(2010, 2011, 2012, 2013, 2014)', '(2010, 2011, 2012, 2013, 2014)', '(2010, 2011, 2012, 2013, 2014)']\n",
      "GT time ranges: ['(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)', '(2019, 2020)']\n",
      "Pred entities: ['Teachta Dála', 'Labour Party', 'Labour Party', 'Labour Party']\n",
      "GT entities: ['Socialist Party', 'RISE']\n",
      "\n",
      "EM: 0.0\n",
      "F1: 0.23636363636363636\n",
      "Recall: 0.30952380952380953\n",
      "Time BLEU: 0.7960980672538887\n",
      "Entity BERT: 0.5890607833862305\n"
     ]
    }
   ],
   "source": [
    "import metrics\n",
    "import utils\n",
    "import importlib\n",
    "import re\n",
    "importlib.reload(metrics)\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "gt_list = gt\n",
    "gt_tokens_list = gt_tokens\n",
    "pred_tokens_list = utils.remove_pad_tokens(output_tokens)\n",
    "pred_list = utils.extract_between_tags(output)\n",
    "pred_list = re.findall(r\"'(.*?)'\", pred_list)  # We convert the output string into list to facilitate metrics computation\n",
    "\n",
    "print(f\"Pred time ranges: {metrics.extract_time_ranges(pred_list)}\")\n",
    "print(f\"GT time ranges: {metrics.extract_time_ranges(gt_list)}\")\n",
    "print(f\"Pred entities: {metrics.extract_entities(pred_list)}\")\n",
    "print(f\"GT entities: {metrics.extract_entities(gt_list)}\")\n",
    "\n",
    "# ---------------------- Syntax-based metrics --------------------------\n",
    "em = metrics.compute_em(gt_list, pred_list)\n",
    "f1 = metrics.compute_f1(gt_tokens_list, pred_tokens_list)\n",
    "recall = metrics.compute_recall(gt_tokens_list, pred_tokens_list)\n",
    "time_bleu = metrics.compute_time_bleu(gt_list, pred_list, tokenizer)  # NOTE: tends to be relatively high, since the time range is already hinted at in the prompt\n",
    "# ---------------------- Semantics-based metrics --------------------------\n",
    "entity_bert = metrics.compute_entity_bert(gt_list, pred_list)   # Uses BERT tokenizer\n",
    "\n",
    "print(f\"\\nEM: {em}\")                              \n",
    "print(f\"F1: {f1}\")                      \n",
    "print(f\"Recall: {recall}\")                           \n",
    "print(f\"Time BLEU: {time_bleu}\")      \n",
    "print(f\"Entity BERT: {entity_bert}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
