{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot TLQA with FlanT5\n",
    "\n",
    "Below, we run generative models FlanT5-large and FlanT5-XL in a few-shot setting by selecting demonstration examples from training set of TLQA, which was derived from tempLLama.\n",
    "- `flan-t5-large`: 780M params, 1GB mem\n",
    "- `flan-t5-xl`: 3B params, 12 GB mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: numpy in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.4.28)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: langchain-core in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (0.1.52)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (0.1.59)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (8.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.37.2 optimum==1.12.0 --quiet\n",
    "!pip install sentence-transformers\n",
    "!pip install langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running FlanT5-Large and FlanT5-XL (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Michael Jackson: The Greatest Hits</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "input_text = \"List all Michael Jackson albums between 2000 and 2009.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212 \n",
      "\n",
      "question: List all entities that owned Linha de Évora from 2010 to 2020.\n",
      "answers: ['REFER (2010, 2011, 2012, 2013, 2014, 2015)', 'Infrastructures of Portugal (2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "type: P127\n",
      "subject: Linha de Évora\n",
      "answer_ids: {'REFER': 'Q1411661', 'Infrastructures of Portugal': 'Q20730106'}\n",
      "wikidata_ID: Q1826620\n",
      "verified: True\n",
      "subject_label: Linha de Évora\n",
      "aliases: []\n",
      "final_answers: ['REFER (2010, 2011, 2012, 2013, 2014, 2015)', 'Infraestruturas de Portugal (2015, 2016, 2017, 2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "train_set = utils.json_to_list(\"../data/train_TLQA.json\")\n",
    "test_set = utils.json_to_list(\"../data/test_TLQA.json\")\n",
    "print(len(train_set), '\\n')\n",
    "example = train_set[42]\n",
    "for key in example.keys():\n",
    "    print(f\"{key}: {example[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Example Generation using KNN Search\n",
    "\n",
    "To select few-shot examples, we select examples from the training set that are close to each test example by using KNN search in a continuous vector space using any embedding model from sentence-transformers.\n",
    "We reuse the KNN search from https://anonymous.4open.science/r/ICAT-47FC/code/2wikimultihopqa/get_chatgpt_skill_transfer_knn.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class KnnSearch:\n",
    "    def __init__(self, data=None, num_trees=None, emb_dim=None):\n",
    "        self.num_trees = num_trees\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def get_embeddings_for_data(self, data_ls):\n",
    "        # NOTE: any embedding model from sentence-transformers can be used\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        embeddings = model.encode(data_ls)\n",
    "        return embeddings\n",
    "\n",
    "    def get_top_n_neighbours(self, sentence, data_emb, transfer_data, k):\n",
    "        \"\"\"\n",
    "        Retrieves the top k most similar questions for \"sentence\" based on cosine similarity from given embeddings \"data_emb\".\n",
    "\n",
    "        Parameters:\n",
    "        sentence (str): The input sentence to find similar questions for.\n",
    "        data_emb (np.ndarray): The embeddings for the transfer questions.\n",
    "        transfer_data (list): The list of transfer questions corresponding to data_emb.\n",
    "        k (int): The number of top similar questions to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of the top k similar questions from transfer_data and all similar questions from str_qa.\n",
    "        \"\"\"\n",
    "        sent_emb = self.get_embeddings_for_data(sentence)\n",
    "        top_questions = []\n",
    "\n",
    "        text_sims = cosine_similarity(data_emb,[sent_emb]).tolist()\n",
    "        results_sims = zip(range(len(text_sims)), text_sims)\n",
    "        sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)  # Obtain the highest similarities\n",
    "\n",
    "        # NOTE: we only match based on questions, but include the full question-answer pair in resulting neighs\n",
    "        for idx, item in sorted_similarities[:k]:\n",
    "                    top_questions.append(transfer_data[idx])\n",
    "\n",
    "        return top_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all questions from the (train) set used for getting neighbours\n",
    "def get_transfer_questions(transfer_data):\n",
    "    transfer_questions = []\n",
    "    for index, data in enumerate(transfer_data):\n",
    "        transfer_questions.append(data[\"question\"])\n",
    "    return transfer_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "knn = KnnSearch()\n",
    "\n",
    "# Read raw dataset to a list containing question and answers (no metadata)\n",
    "train_data = json_to_list(\"../data/train_TLQA.json\")\n",
    "# Keep questions only to embed (to use in similarity metric)\n",
    "train_questions = get_transfer_questions(train_data)\n",
    "train_questions_emb = knn.get_embeddings_for_data(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all positions Per Sandberg held from 2015 to 2018.\n",
      "['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']\n",
      "\n",
      "\n",
      "List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n",
      "\n",
      "\n",
      "List all positions David M. O'Connell held from 2010 to 2020.\n",
      "['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: loop over test samples during/before inference time to create few-shot prompts\n",
    "# question = \"List all employers Elon Musk worked for from 1990 to 2024.\"\n",
    "question = \"List all positions held by Mark Zuckerberg from 2000 to 2024.\"\n",
    "neighs = knn.get_top_n_neighbours(sentence=question, data_emb=train_questions_emb, transfer_data=train_data, k=3)   # data_emb is embedded questions only, so we only match questions\n",
    "for neigh in neighs:\n",
    "    print(neigh['question'])\n",
    "    print(neigh['answers'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Few-Shot Prompt Template using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'List all positions Per Sandberg held from 2015 to 2018.', 'answers': ['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']}, {'question': 'List all positions Mark Drakeford held from 2013 to 2020.', 'answers': ['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']}, {'question': \"List all positions David M. O'Connell held from 2010 to 2020.\", 'answers': ['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']}]\n"
     ]
    }
   ],
   "source": [
    "def simplify_dict_list(dict_list):\n",
    "    return [{'question': item['question'], 'answers': item['answers']} for item in dict_list]\n",
    "\n",
    "simple_neighs = simplify_dict_list(neighs)\n",
    "print(simple_neighs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we configure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answers\"], template=\"Question: {question}\\n{answers}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**simple_neighs[1]))\n",
    "# print(example_prompt.format(question=f\"{simple_neighs[0]['question']}\", answers=f\"{simple_neighs[0]['answers']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed examples and formatter to FewShotPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List all positions Per Sandberg held from 2015 to 2018.\n",
      "['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']\n",
      "\n",
      "Question: List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions David M. O'Connell held from 2010 to 2020.\n",
      "['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions held by Mark Zuckerberg between 2000 and 2020. Please answer this question in the same format as the three examples above.\n"
     ]
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=simple_neighs,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "few_shot_prompt = prompt.format(input=\"List all positions held by Mark Zuckerberg between 2000 and 2020. Please answer this question in the same format as the three examples above.\")\n",
    "print(few_shot_prompt)   # String to feed to model during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLQA Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "# TODO: uncomment when using GPU\n",
    "# import accelerate\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Uncomment the line below to run on GPU\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  784,    31, 22081,   989,   907,   377,     5,   254,     5,    41,\n",
      "         14926,     6,  8558,  1673,    61,    31,     6,     3,    31, 14337,\n",
      "          1926,   545,   377,     5,   254,     5,    41, 12172,     6,  2038,\n",
      "            61,    31,     6,     3,    31,   254,    60,  1123, 10457,     9,\n",
      "           377,     5,   254,     5,    41, 11138,     6,  1412,     6,  1230,\n",
      "            61,    31,     6,     3,    31, 14714,  3833,    15,   377,     5,\n",
      "           254,     5,    41,  8651,     6,  5123,  4791,  4323,  7887,  6503,\n",
      "            61,    31,   908,     1]])\n",
      "['Southend United F.C. (2010, 2011, 2012)', 'Stevenage F.C. (2012, 2013)', 'Crewe Alexandra F.C. (2013, 2014, 2015)', 'Port Vale F.C. (2015, 2016, 2017, 2018, 2019, 2020)']</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  784,    31,  7855,   526,  3271,    13, 11897, 26118,    31,     6,\n",
      "             3,    31, 25171,     3, 16911,  5923,  3271,    13, 11897, 26118,\n",
      "            31,     6,    96, 24337,    31,     7,     3, 16911,    13, 11897,\n",
      "            41, 12172,     6,  7218,  1412,     6,  1230,    61,  1686,     3,\n",
      "            31,   254, 19176,   348,    13,     8,   781,   157, 23304,    29,\n",
      "             9,  6324,     9,    41, 10218,     6,  1230,     6,  5123,  4791,\n",
      "          4323,  7887,  6503,    61,    31,     6,     3,    31,   345, 15704,\n",
      "            13, 11897, 24373,    31,   908,     1]])\n",
      "['Prime Minister of Ukraine (2010)', 'First Deputy Prime Minister of Ukraine (2010)', \"People's Deputy of Ukraine (2012, 2013, 2014, 2015)\", 'Chairman of the Verkhovna Rada (2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'President of Ukraine (2014)']</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  784,    31,  5231,  4703,   343,  3450,    41, 14926,     6,  8558,\n",
      "          1673,     6,  7218,  1412,     6,  1230,     6,  5123,  4791,  4323,\n",
      "          1360,    61,    31,     6,     3,    31, 13431,   427,    41,   196,\n",
      "            60,    40,   232,    61,    41,  8584,     6,  6503,    61,    31,\n",
      "           908,     1]])\n",
      "['Socialist Party (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)', 'RISE (Ireland) (2019, 2020)']</s>\n"
     ]
    }
   ],
   "source": [
    "K = 10   # TODO: Experiment with 3, 5, 7, 10\n",
    "# TODO: Compute metrics over these results later\n",
    "# TODO: Store in Huggingface Dataset\n",
    "results_GT_dict = {'prompts': [], 'outputs': [], 'output_tokens': [], \n",
    "                            'ground_truths': [], 'ground_truth_tokens': []}  \n",
    "\n",
    "# Configure formatter that will format the few-shot examples into a string\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answers\"], template=\"Question: {question}\\n{answers}\"\n",
    ")\n",
    "\n",
    "# Convert test set to list and loop over all items (1071 in total)\n",
    "for i, item in enumerate(test_set):\n",
    "\n",
    "    if i == 3:\n",
    "        break\n",
    "    \n",
    "    # For each test question, retrieve k neighbours (try 3, 5, 7, 10)\n",
    "    test_question = test_set[i]['question']\n",
    "    neighs = knn.get_top_n_neighbours(sentence=test_question, data_emb=train_questions_emb, transfer_data=train_data, k=K)\n",
    "    simple_neighs = simplify_dict_list(neighs)\n",
    "\n",
    "    # Create the few-shot prompt template and feed to model\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=simple_neighs,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"Question: {input}\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "    few_shot_prompt = prompt.format(input=f\"{test_question} Please answer this question in the same format as the {K} examples above.\")\n",
    "    results_GT_dict['prompts'].append(few_shot_prompt)\n",
    "    \n",
    "    # TODO: pre-tokenize input and GT to speed up inference\n",
    "    input_ids = tokenizer(few_shot_prompt, return_tensors=\"pt\").input_ids\n",
    "    # input_ids = tokenizer(few_shot_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")  # TODO: uncomment when using GPU\n",
    "    output_tokens = model.generate(input_ids, max_length=200)\n",
    "    output = tokenizer.decode(output_tokens[0])\n",
    "\n",
    "    results_GT_dict['output_tokens'].append(output_tokens[0])\n",
    "    results_GT_dict['outputs'].append(output)\n",
    "    results_GT_dict['ground_truths'].append(test_set[i]['final_answers'])\n",
    "    gt_tokens = tokenizer(str(test_set[i]['final_answers']), return_tensors=\"pt\").input_ids\n",
    "    results_GT_dict['ground_truth_tokens'].append(gt_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we inspect individual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions made: 3\n",
      "\n",
      "\n",
      "PROMPT (idx = 1):\n",
      "Question: List all positions Venedykt Aleksiichuk, also known as Venedykt Aleksiychuk, held from 2010 to 2020.\n",
      "['auxiliary bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'titular bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'diocesan bishop (2017, 2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions Arseniy Yatsenyuk, also known as Arseniy Petrovych Yatsenyuk, held from 2010 to 2016.\n",
      "[\"People's Deputy of Ukraine (2010, 2011, 2012, 2013, 2014)\", 'Prime Minister of Ukraine (2014, 2015, 2016)']\n",
      "\n",
      "Question: List all positions Mykola Azarov, also known as Mykola Yanovych Azarov, held from 2010 to 2014.\n",
      "[\"People's Deputy of Ukraine (2010, 2012)\", 'Prime Minister of Ukraine (2010, 2011, 2012, 2013, 2014)']\n",
      "\n",
      "Question: List all positions Mikhail Mishustin, also known as Mikhail Vladimirovich Mishustin, held from 2010 to 2020.\n",
      "['director (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Prime Minister of Russia (2020)']\n",
      "\n",
      "Question: List all positions Petro Poroshenko, also known as Petro Oleksiyovych Poroshenko, held from 2010 to 2020.\n",
      "['Minister of Foreign Affairs (2010)', \"People's Deputy of Ukraine (2012, 2013, 2014, 2019, 2020)\", 'President of Ukraine (2014, 2015, 2016, 2017, 2018, 2019)']\n",
      "\n",
      "Question: List all positions László Trócsányi, also known as Laszlo Trocsanyi, held from 2014 to 2020.\n",
      "['Minister of Justice of Hungary (2014, 2015, 2016, 2017, 2018, 2019)', 'member of the European Parliament (2019, 2020)', 'president (2020)']\n",
      "\n",
      "Question: List all positions Milorad Dodik held from 2010 to 2020.\n",
      "['President of Republika Srpska (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018)', 'Prime Minister of Republika Srpska (2010)', 'Chairmen of the Presidency of Bosnia and Herzegovina (2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions Mikheil Saakashvili, also known as Mikhail Saakashvili, held from 2010 to 2016.\n",
      "['President of Georgia (2010, 2011, 2012, 2013)', 'Governor of Odessa Oblast (2015, 2016)']\n",
      "\n",
      "Question: List all positions Dmitry Medvedev, also known as Dima Medvedev, held from 2010 to 2020.\n",
      "['President of Russia (2010, 2011, 2012)', 'Prime Minister of Russia (2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions Vyacheslav Volodin, also known as Vyacheslav Viktorovich Volodin, held from 2010 to 2020.\n",
      "['Deputy Chairman of the Government of the Russian Federation (2010, 2011)', 'Chairman of the State Duma (2016, 2017, 2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions Oleksandr Turchynov, also known as Oleksandr Valentynovych Turchynov, held from 2010 to 2020. Please answer this question in the same format as the 10 examples above.\n",
      "\n",
      "\n",
      "MODEL OUTPUT:\n",
      "<pad> ['Deputy Prime Minister of Ukraine (2010, 2011)', 'Deputy Chairman of the Board of the National Bank of Ukraine (2010, 2011)', 'Deputy Chairman of the Board of the National Bank of Ukraine (2010, 2011)', 'Deputy Chairman of the Board of the National Bank of Ukraine (2010, 2011)']</s>\n",
      "\n",
      "\n",
      "GROUND TRUTH:\n",
      "['Prime Minister of Ukraine (2010)', 'First Deputy Prime Minister of Ukraine (2010)', \"People's Deputy of Ukraine (2012, 2013, 2014, 2015)\", 'Chairman of the Verkhovna Rada (2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'President of Ukraine (2014)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_to_inspect = 1\n",
    "print(f\"Total predictions made: {len(prompts_results_GT_dict)}\\n\")\n",
    "\n",
    "example_prompt = prompts_results_GT_dict['prompts'][idx_to_inspect]\n",
    "example_output = prompts_results_GT_dict['outputs'][idx_to_inspect]\n",
    "example_gt = prompts_results_GT_dict['ground_truths'][idx_to_inspect]\n",
    "\n",
    "print(f\"\\nPROMPT (idx = {idx_to_inspect}):\\n{example_prompt}\\n\")\n",
    "print(f\"\\nMODEL OUTPUT:\\n{example_output}\\n\")\n",
    "print(f\"\\nGROUND TRUTH:\\n{example_gt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will test the utility functions that compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['Deputy Prime Minister of Ukraine (2010, 2011)', 'Deputy Chairman of the Board of the National Bank of Ukraine (2010, 2011)', 'Deputy Chairman of the Board of the National Bank of Ukraine (2010, 2011)', 'Deputy Chairman of the Board of the National Bank of Ukraine (2010, 2011)']\n",
      "GROUND TRUTH: ['Prime Minister of Ukraine (2010)', 'First Deputy Prime Minister of Ukraine (2010)', \"People's Deputy of Ukraine (2012, 2013, 2014, 2015)\", 'Chairman of the Verkhovna Rada (2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'President of Ukraine (2014)']\n",
      "\n",
      "Pred time ranges: ['(2010, 2011)', '(2010, 2011)', '(2010, 2011)', '(2010, 2011)']\n",
      "GT time ranges: ['(2010)', '(2010)', '(2012, 2013, 2014, 2015)', '(2014, 2015, 2016, 2017, 2018, 2019, 2020)', '(2014)']\n",
      "Pred entities: ['Deputy Prime Minister of Ukraine', 'Deputy Chairman of the Board of the National Bank of Ukraine', 'Deputy Chairman of the Board of the National Bank of Ukraine', 'Deputy Chairman of the Board of the National Bank of Ukraine']\n",
      "GT entities: ['Prime Minister of Ukraine', 'First Deputy Prime Minister of Ukraine', \"People's Deputy of Ukraine\", 'Chairman of the Verkhovna Rada', 'President of Ukraine']\n",
      "\n",
      "EM: 0.0\n",
      "F1: 0.0\n",
      "Recall: 0.0\n",
      "Time precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "import compute_metrics\n",
    "import utils\n",
    "import importlib\n",
    "import re\n",
    "importlib.reload(compute_metrics)\n",
    "importlib.reload(utils)\n",
    "\n",
    "example_pred = utils.extract_between_tags(example_output)\n",
    "example_pred_list = re.findall(r\"'(.*?)'\", example_pred)  # NOTE: we convert the output string into list to facilitate metrics computation\n",
    "print(f\"PRED: {example_pred_list}\")\n",
    "print(f\"GROUND TRUTH: {example_gt}\\n\")\n",
    "\n",
    "print(f\"Pred time ranges: {compute_metrics.extract_time_ranges(example_pred_list)}\")\n",
    "print(f\"GT time ranges: {compute_metrics.extract_time_ranges(example_gt)}\")\n",
    "print(f\"Pred entities: {compute_metrics.extract_entities(example_pred_list)}\")\n",
    "print(f\"GT entities: {compute_metrics.extract_entities(example_gt)}\")\n",
    "\n",
    "# TODO: evaluate on list of tokenized items (Entity-Time pairs) <-- add extra col to results Dataset\n",
    "\n",
    "# ---------------------- Syntax-based metrics --------------------------\n",
    "\n",
    "# example_pred_list.append('Prime Minister of Ukraine (2010)')\n",
    "\n",
    "em = compute_metrics.compute_em(example_gt, example_pred_list)\n",
    "\n",
    "# F1: word overlap between the labeled and the predicted answer\n",
    "# TODO: For both precision and recall, match on TOKENS per Entity-Time pair and average\n",
    "f1 = compute_metrics.compute_f1_score(example_gt, example_pred_list)\n",
    "\n",
    "# Completeness: Recall = # correct answers / # GT answers \n",
    "recall = compute_metrics.compute_recall(example_gt, example_pred_list)\n",
    "\n",
    "# TimePrecision = # correct time ranges / # time ranges given\n",
    "# TODO: Precision_time: over time ranges (NOT tokenized, as we wanna match on full year)\n",
    "time_precision = compute_metrics.compute_time_precision_score(example_gt, example_pred_list)\n",
    "\n",
    "# EntityPrecision = # correct entities / # entites given\n",
    "# entity_precision = compute_metrics.compute_entity_precision_score(example_gt, example_pred_list)\n",
    "\n",
    "# TODO: BLEU_entity: over entity items (tokenized and concatenated) -> use sentence_blue or corpus_bleu\n",
    "# It calculates a precision score for each n-gram size (typically 1 to 4) and then computes a geometric mean of these precisions.\n",
    "# entity_bleu_score = corpus_bleu(example_gt, example_pred_list)\n",
    "# print(entity_bleu_score)\n",
    "\n",
    "# ---------------------- Semantics-based metrics  (only for entities) --------------------------\n",
    "\n",
    "# TODO: BertScore_entity (or METEOR): over entity items (tokenized and concatenated)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nEM: {em}\")                              \n",
    "print(f\"F1: {f1}\")                                  \n",
    "print(f\"Recall: {recall}\")                           \n",
    "print(f\"Time precision: {time_precision}\")          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
