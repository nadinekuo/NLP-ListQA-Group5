{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot TLQA with FlanT5\n",
    "\n",
    "Below, we run generative models FlanT5-large and FlanT5-XL in a few-shot setting by selecting demonstration examples from training set of TLQA, which was derived from tempLLama.\n",
    "- `flan-t5-large`: 780M params, 1GB mem\n",
    "- `flan-t5-xl`: 3B params, 12 GB mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: numpy in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.4.28)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: langchain-core in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (0.1.52)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (0.1.59)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langchain-core) (8.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nadinekuo/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.37.2 optimum==1.12.0 --quiet\n",
    "!pip install sentence-transformers\n",
    "!pip install langchain-core\n",
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running FlanT5-Large and FlanT5-XL (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Michael Jackson: The Greatest Hits</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "input_text = \"List all Michael Jackson albums between 2000 and 2009.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3212 \n",
      "\n",
      "question: List all entities that owned Linha de Évora from 2010 to 2020.\n",
      "answers: ['REFER (2010, 2011, 2012, 2013, 2014, 2015)', 'Infrastructures of Portugal (2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "type: P127\n",
      "subject: Linha de Évora\n",
      "answer_ids: {'REFER': 'Q1411661', 'Infrastructures of Portugal': 'Q20730106'}\n",
      "wikidata_ID: Q1826620\n",
      "verified: True\n",
      "subject_label: Linha de Évora\n",
      "aliases: []\n",
      "final_answers: ['REFER (2010, 2011, 2012, 2013, 2014, 2015)', 'Infraestruturas de Portugal (2015, 2016, 2017, 2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "train_set = utils.json_to_list(\"../data/train_TLQA.json\")\n",
    "test_set = utils.json_to_list(\"../data/test_TLQA.json\")\n",
    "print(len(train_set), '\\n')\n",
    "example = train_set[45]\n",
    "for key in example.keys():\n",
    "    print(f\"{key}: {example[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Example Generation using KNN Search\n",
    "\n",
    "To select few-shot examples, we select examples from the training set that are close to each test example by using KNN search in a continuous vector space using any embedding model from sentence-transformers.\n",
    "We reuse the KNN search from https://anonymous.4open.science/r/ICAT-47FC/code/2wikimultihopqa/get_chatgpt_skill_transfer_knn.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class KnnSearch:\n",
    "    def __init__(self, data=None, num_trees=None, emb_dim=None):\n",
    "        self.num_trees = num_trees\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def get_embeddings_for_data(self, data_ls):\n",
    "        # NOTE: any embedding model from sentence-transformers can be used\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        embeddings = model.encode(data_ls)\n",
    "        return embeddings\n",
    "\n",
    "    def get_top_n_neighbours(self, sentence, data_emb, transfer_data, k):\n",
    "        \"\"\"\n",
    "        Retrieves the top k most similar questions for \"sentence\" based on cosine similarity from given embeddings \"data_emb\".\n",
    "\n",
    "        Parameters:\n",
    "        sentence (str): The input sentence to find similar questions for.\n",
    "        data_emb (np.ndarray): The embeddings for the transfer questions.\n",
    "        transfer_data (list): The list of transfer questions corresponding to data_emb.\n",
    "        k (int): The number of top similar questions to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of the top k similar questions from transfer_data and all similar questions from str_qa.\n",
    "        \"\"\"\n",
    "        sent_emb = self.get_embeddings_for_data(sentence)\n",
    "        top_questions = []\n",
    "\n",
    "        text_sims = cosine_similarity(data_emb,[sent_emb]).tolist()\n",
    "        results_sims = zip(range(len(text_sims)), text_sims)\n",
    "        sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)  # Obtain the highest similarities\n",
    "\n",
    "        # NOTE: we only match based on questions, but include the full question-answer pair in resulting neighs\n",
    "        for idx, item in sorted_similarities[:k]:\n",
    "                    top_questions.append(transfer_data[idx])\n",
    "\n",
    "        return top_questions\n",
    "    \n",
    "    def semantic_score(self, question, document):\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        query_embedding = model.encode(question)\n",
    "        document_embeddings = model.encode(document)\n",
    "\n",
    "        return np.dot(query_embedding, document_embeddings)\n",
    "\n",
    "    def semantic_values(self, questions, documents):\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        query_embedding = model.encode(questions)\n",
    "        document_embeddings = model.encode(documents)\n",
    "\n",
    "        semantic_scores = []\n",
    "        for question_emb in query_embedding:\n",
    "            for document_emb in document_embeddings:\n",
    "                semantic_scores.append(self.semantic_score(question_emb, document_emb))\n",
    "\n",
    "        return np.mean(np.array(semantic_scores)), np.std(np.array(semantic_scores))\n",
    "\n",
    "    def temporal_score(self, sentence_timestamp: datetime, data_emb_timestamp: datetime):\n",
    "        alpha = 1.0\n",
    "        return alpha / (data_emb_timestamp.timestamp() - sentence_timestamp.timestamp())\n",
    "\n",
    "    def get_top_n_temp_neighbours(self, sentence_timestamp: datetime, data_emb_timestamps: list[datetime],\n",
    "                                  standard_deviation_sematic, mean_sematic, transfer_data, k):\n",
    "        \"\"\"\n",
    "        Retrieves the top k most similar questions for \"sentence\" based on cosine similarity from given embeddings \"data_emb\".\n",
    "\n",
    "        Parameters:\n",
    "        sentence (str): The input sentence to find similar questions for.\n",
    "        data_emb (np.ndarray): The embeddings for the transfer questions.\n",
    "        transfer_data (list): The list of transfer questions corresponding to data_emb.\n",
    "        k (int): The number of top similar questions to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of the top k similar questions from transfer_data and all similar questions from str_qa.\n",
    "        \"\"\"\n",
    "        timestamps = np.array([dt.timestamp() for dt in data_emb_timestamps])\n",
    "        mean_timestamp = np.mean(timestamps)\n",
    "        standard_deviation_timestamp = np.std(timestamps)\n",
    "\n",
    "        temp_scores = []\n",
    "        for data_emb_timestamp in data_emb_timestamps:\n",
    "            temp_score = ((self.temporal_score(sentence_timestamp,\n",
    "                                               data_emb_timestamp) - mean_timestamp) / standard_deviation_timestamp) * standard_deviation_sematic + mean_sematic\n",
    "            temp_scores.append(temp_score)\n",
    "\n",
    "        results_sims = zip(range(len(temp_scores)), temp_scores)\n",
    "        sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_questions = []\n",
    "        for idx, item in sorted_similarities[:k]:\n",
    "            top_questions.append(transfer_data[idx])\n",
    "\n",
    "        return top_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all questions from the (train) set used for getting neighbours\n",
    "def get_transfer_questions(transfer_data):\n",
    "    transfer_questions = []\n",
    "    for index, data in enumerate(transfer_data):\n",
    "        transfer_questions.append(data[\"question\"])\n",
    "    return transfer_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "knn = KnnSearch()\n",
    "\n",
    "# Read raw dataset to a list containing question and answers (no metadata)\n",
    "train_data = json_to_list(\"../data/train_TLQA.json\")\n",
    "# Keep questions only to embed (to use in similarity metric)\n",
    "train_questions = get_transfer_questions(train_data)\n",
    "train_questions_emb = knn.get_embeddings_for_data(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all positions Per Sandberg held from 2015 to 2018.\n",
      "['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']\n",
      "\n",
      "\n",
      "List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n",
      "\n",
      "\n",
      "List all positions David M. O'Connell held from 2010 to 2020.\n",
      "['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "# TODO: loop over test samples during/before inference time to create few-shot prompts\n",
    "# question = \"List all employers Elon Musk worked for from 1990 to 2024.\"\n",
    "question = \"List all positions held by Mark Zuckerberg from 2000 to 2024.\"\n",
    "neighs = knn.get_top_n_neighbours(sentence=question, data_emb=train_questions_emb, transfer_data=train_data, k=3)   # data_emb is embedded questions only, so we only match questions\n",
    "for neigh in neighs:\n",
    "    print(neigh['question'])\n",
    "    print(neigh['answers'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Few-Shot Prompt Template using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'List all positions Per Sandberg held from 2015 to 2018.', 'answers': ['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']}, {'question': 'List all positions Mark Drakeford held from 2013 to 2020.', 'answers': ['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']}, {'question': \"List all positions David M. O'Connell held from 2010 to 2020.\", 'answers': ['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']}]\n"
     ]
    }
   ],
   "source": [
    "def simplify_dict_list(dict_list):\n",
    "    return [{'question': item['question'], 'answers': item['answers']} for item in dict_list]\n",
    "\n",
    "simple_neighs = simplify_dict_list(neighs)\n",
    "print(simple_neighs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we configure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answers\"], template=\"Question: {question}\\n{answers}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**simple_neighs[1]))\n",
    "# print(example_prompt.format(question=f\"{simple_neighs[0]['question']}\", answers=f\"{simple_neighs[0]['answers']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed examples and formatter to FewShotPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List all positions Per Sandberg held from 2015 to 2018.\n",
      "['Minister of Fisheries (2015, 2016, 2017, 2018)', 'Minister of Justice and Public Security (2017, 2018)']\n",
      "\n",
      "Question: List all positions Mark Drakeford held from 2013 to 2020.\n",
      "['Minister for Health and Social Services (2013, 2014, 2015, 2016)', 'First Minister of Wales (2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions David M. O'Connell held from 2010 to 2020.\n",
      "['diocesan bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)', 'Catholic bishop (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)']\n",
      "\n",
      "Question: List all positions held by Mark Zuckerberg between 2000 and 2020. Please answer this question in the same format as the three examples above.\n"
     ]
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=simple_neighs,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "few_shot_prompt = prompt.format(input=\"List all positions held by Mark Zuckerberg between 2000 and 2020. Please answer this question in the same format as the three examples above.\")\n",
    "print(few_shot_prompt)   # String to feed to model during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLQA Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "# TODO: uncomment when using GPU\n",
    "# import accelerate\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Uncomment the line below to run on GPU\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/nadinekuo/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5/5 [00:00<00:00, 1595.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "K = 10   # TODO: Experiment with 3, 5, 7, 10\n",
    "MAX_OUTPUT_LEN = 200\n",
    "MODEL_NAME = \"flan-t5-large\"  # TODO: Set accordingly\n",
    "\n",
    "results_GT_dict = {'prompts': [], 'outputs': [], 'output_tokens': [], \n",
    "                            'ground_truths': [], 'ground_truth_tokens': []}  \n",
    "\n",
    "# Configure formatter that will format the few-shot examples into a string\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answers\"], template=\"Question: {question}\\n{answers}\"\n",
    ")\n",
    "\n",
    "# Convert test set to list and loop over all items (1071 in total)\n",
    "for i, item in enumerate(test_set):\n",
    "    \n",
    "    # For each test question, retrieve k neighbours (try 3, 5, 7, 10)\n",
    "    test_question = test_set[i]['question']\n",
    "    neighs = knn.get_top_n_neighbours(sentence=test_question, data_emb=train_questions_emb, transfer_data=train_data, k=K)\n",
    "    simple_neighs = simplify_dict_list(neighs)\n",
    "\n",
    "    # Create the few-shot prompt template and feed to model\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=simple_neighs,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"Question: {input}\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "    few_shot_prompt = prompt.format(input=f\"{test_question} Please answer this question in the same format as the {K} examples above.\")\n",
    "    results_GT_dict['prompts'].append(few_shot_prompt)\n",
    "    \n",
    "    # TODO: pre-tokenize input and GT to speed up inference\n",
    "    input_ids = tokenizer(few_shot_prompt, return_tensors=\"pt\").input_ids\n",
    "    # TODO: ---------------------- UNCOMMENT WHEN USING GPU -----------------------------\n",
    "    # input_ids = tokenizer(few_shot_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    output_tokens = model.generate(input_ids, max_length=MAX_OUTPUT_LEN)\n",
    "    output = tokenizer.decode(output_tokens[0])\n",
    "\n",
    "    results_GT_dict['output_tokens'].append(output_tokens[0])\n",
    "    results_GT_dict['outputs'].append(output)\n",
    "    results_GT_dict['ground_truths'].append(test_set[i]['final_answers'])\n",
    "    gt_tokens = tokenizer(str(test_set[i]['final_answers']), return_tensors=\"pt\").input_ids[0]\n",
    "    results_GT_dict['ground_truth_tokens'].append(gt_tokens)\n",
    "\n",
    "results_ds = Dataset.from_dict(results_GT_dict)\n",
    "results_ds.save_to_disk(f\"{K}_shot_{MODEL_NAME}.hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the results Dataset from disk and inspect individual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompts', 'outputs', 'output_tokens', 'ground_truths', 'ground_truth_tokens'],\n",
      "    num_rows: 5\n",
      "})\n",
      "\n",
      "MODEL OUTPUT:\n",
      "<pad> ['Teachta Dála (2010, 2012, 2013, 2014)', 'Labour Party (2010, 2011, 2012, 2013, 2014)', 'Labour Party (2010, 2011, 2012, 2013, 2014)', 'Labour Party (2010, 2011, 2012, 2013, 2014)']</s>\n",
      "\n",
      "OUTPUT TOKENS:\n",
      "[0, 784, 31, 382, 15, 9, 3997, 9, 309, 2975, 521, 41, 14926, 6, 1673, 6, 7218, 1412, 61, 31, 6, 3, 31, 18506, 1211, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 61, 31, 6, 3, 31, 18506, 1211, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 61, 31, 6, 3, 31, 18506, 1211, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 61, 31, 908, 1]\n",
      "\n",
      "GROUND TRUTH:\n",
      "['Socialist Party (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)', 'RISE (Ireland) (2019, 2020)']\n",
      "\n",
      "GT TOKENS:\n",
      "[784, 31, 5231, 4703, 343, 3450, 41, 14926, 6, 8558, 1673, 6, 7218, 1412, 6, 1230, 6, 5123, 4791, 4323, 1360, 61, 31, 6, 3, 31, 13431, 427, 41, 196, 60, 40, 232, 61, 41, 8584, 6, 6503, 61, 31, 908, 1]\n"
     ]
    }
   ],
   "source": [
    "results_ds = Dataset.load_from_disk(\"../results/10_shot_flan-t5-large_test.hf\")\n",
    "print(results_ds)\n",
    "idx_to_inspect = 2\n",
    "\n",
    "prompt = results_ds['prompts'][idx_to_inspect]\n",
    "output = results_ds['outputs'][idx_to_inspect]\n",
    "output_tokens = results_ds['output_tokens'][idx_to_inspect]\n",
    "gt_tokens = results_ds['ground_truth_tokens'][idx_to_inspect]\n",
    "gt = results_ds['ground_truths'][idx_to_inspect]\n",
    "\n",
    "# print(f\"\\nPROMPT (idx = {idx_to_inspect}):\\n{example_prompt}\\n\")\n",
    "print(f\"\\nMODEL OUTPUT:\\n{output}\\n\\nOUTPUT TOKENS:\\n{output_tokens}\")\n",
    "print(f\"\\nGROUND TRUTH:\\n{gt}\\n\\nGT TOKENS:\\n{gt_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will test the utility functions that compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred time ranges: ['(2010, 2012, 2013, 2014)', '(2010, 2011, 2012, 2013, 2014)', '(2010, 2011, 2012, 2013, 2014)', '(2010, 2011, 2012, 2013, 2014)']\n",
      "GT time ranges: ['(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)', '(2019, 2020)']\n",
      "Pred entities: ['Teachta Dála', 'Labour Party', 'Labour Party', 'Labour Party']\n",
      "GT entities: ['Socialist Party', 'RISE']\n",
      "\n",
      "EM: 0.0\n",
      "F1: 0.23636363636363636\n",
      "Recall: 0.30952380952380953\n",
      "Time BLEU: 0.7960980672538887\n",
      "Entity BERT: 0.5890607833862305\n"
     ]
    }
   ],
   "source": [
    "import metrics\n",
    "import utils\n",
    "import importlib\n",
    "import re\n",
    "importlib.reload(metrics)\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "gt_list = gt\n",
    "gt_tokens_list = gt_tokens\n",
    "pred_tokens_list = utils.remove_pad_tokens(output_tokens)\n",
    "pred_list = utils.extract_between_tags(output)\n",
    "pred_list = re.findall(r\"'(.*?)'\", pred_list)  # We convert the output string into list to facilitate metrics computation\n",
    "\n",
    "print(f\"Pred time ranges: {metrics.extract_time_ranges(pred_list)}\")\n",
    "print(f\"GT time ranges: {metrics.extract_time_ranges(gt_list)}\")\n",
    "print(f\"Pred entities: {metrics.extract_entities(pred_list)}\")\n",
    "print(f\"GT entities: {metrics.extract_entities(gt_list)}\")\n",
    "\n",
    "# ---------------------- Syntax-based metrics --------------------------\n",
    "em = metrics.compute_em(gt_list, pred_list)\n",
    "f1 = metrics.compute_f1(gt_tokens_list, pred_tokens_list)\n",
    "recall = metrics.compute_recall(gt_tokens_list, pred_tokens_list)\n",
    "time_bleu = metrics.compute_time_bleu(gt_list, pred_list, tokenizer)  # NOTE: tends to be relatively high, since the time range is already hinted at in the prompt\n",
    "# ---------------------- Semantics-based metrics --------------------------\n",
    "entity_bert = metrics.compute_entity_bert(gt_list, pred_list)   # Uses BERT tokenizer\n",
    "\n",
    "print(f\"\\nEM: {em}\")                              \n",
    "print(f\"F1: {f1}\")                      \n",
    "print(f\"Recall: {recall}\")                           \n",
    "print(f\"Time BLEU: {time_bleu}\")      \n",
    "print(f\"Entity BERT: {entity_bert}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
