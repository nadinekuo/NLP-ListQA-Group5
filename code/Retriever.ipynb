{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8521096,"sourceType":"datasetVersion","datasetId":5087826},{"sourceId":8521380,"sourceType":"datasetVersion","datasetId":5088021}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-26T11:04:34.616462Z","iopub.execute_input":"2024-05-26T11:04:34.617213Z","iopub.status.idle":"2024-05-26T11:04:47.957403Z","shell.execute_reply.started":"2024-05-26T11:04:34.617178Z","shell.execute_reply":"2024-05-26T11:04:47.956246Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.7.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Flan-T5-large + tas-b**","metadata":{}},{"cell_type":"code","source":"import json\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load infoboxes\n# Update the file path to the location where you uploaded the file in Kaggle\nwith open('/kaggle/input/infoboxes-v1/extracted_infoboxes.json') as f:\n    infoboxes = json.load(f)\n\n# Extract relevant text from infoboxes and create a searchable corpus\ncorpus = []\nfor item in infoboxes:\n    title = item.get(\"title\", \"\")\n    infobox_text = item.get(\"infobox\", \"\")\n    if title and infobox_text:\n        corpus.append(f\"{title}: {infobox_text}\")\n\n# Initialize the retriever model\nretriever = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\ncorpus_embeddings = retriever.encode(corpus, convert_to_tensor=True)\n\n# Function to retrieve top-k context\ndef retrieve_context(query, top_k):\n    query_embedding = retriever.encode(query, convert_to_tensor=True)\n    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n    return [corpus[hit['corpus_id']] for hit in hits[0]]\n\n# Initialize the generative model\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n\n# Function to generate answer using the retrieved context\ndef generate_answer(query, top_k):\n    contexts = retrieve_context(query, top_k=top_k)\n    \n    # Clean and format the contexts\n    contexts_cleaned = []\n    for context in contexts:\n        context_cleaned = context.replace(\"[[\", \"\").replace(\"]]\", \"\").replace(\"'''\", \"\").replace(\"|\", \": \")\n        context_cleaned = context_cleaned.replace(\"\\n\", \" \").replace(\"  \", \" \")\n        contexts_cleaned.append(context_cleaned)\n    \n    combined_context = \" \".join(contexts_cleaned)\n    \n    # Ensure combined context does not exceed max length\n    max_length = 512\n    combined_tokens = tokenizer.encode(combined_context)\n    if len(combined_tokens) > max_length:\n        combined_tokens = combined_tokens[:max_length]\n        combined_context = tokenizer.decode(combined_tokens, skip_special_tokens=True)\n    \n    input_text = f\"Context: {combined_context}\\n\\nQuestion: {query}\\n\\nAnswer (please format as a timeline):\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n    outputs = model.generate(inputs.input_ids, max_length=200)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\n# Function to run experiments with different top_k values\ndef run_experiments(query):\n    for top_k in [3, 5, 7, 10]:\n        print(f\"Running experiment with top_k={top_k}\")\n        answer = generate_answer(query, top_k)\n        print(f\"Answer with top_k={top_k}:\\n{answer}\\n\")\n\n# Test the system with the sample question\nquery = \"What were the major historical periods of Nauru and their respective timelines?\"\nrun_experiments(query)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T11:15:59.138021Z","iopub.execute_input":"2024-05-26T11:15:59.138859Z","iopub.status.idle":"2024-05-26T11:17:33.514282Z","shell.execute_reply.started":"2024-05-26T11:15:59.138824Z","shell.execute_reply":"2024-05-26T11:17:33.513311Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54027251612e4e1ebd187c0667e4d1aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93a4d961b9dd4d9e846aad3468b59f76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0383dabf724c88bfcc11213fceb438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd14b207df64482a2964e07f602d065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a474ff366784b6393efc1bde142cd30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31f3454bc9446e29faf31abe07381e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd2cc9dc96324135838b00cbacf28d6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa0cc47726bd43cb9e9edf592739968c"}},"metadata":{}},{"name":"stdout","text":"Running experiment with top_k=3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494c3b61d1344e0291c2e885c948013d"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (947 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Answer with top_k=3:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\nRunning experiment with top_k=5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c54cc10de7db461e9291b17dabc1957f"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=5:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\nRunning experiment with top_k=7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9258f63489084544be31b509879d0d11"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=7:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\nRunning experiment with top_k=10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de364c682540489a8aa68101708ee911"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=10:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Flan-T5-XL + tas-b**","metadata":{}},{"cell_type":"code","source":"import json\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load infoboxes\n# Update the file path to the location where you uploaded the file in Kaggle\nwith open('/kaggle/input/infoboxes-v1/extracted_infoboxes.json') as f:\n    infoboxes = json.load(f)\n\n# Extract relevant text from infoboxes and create a searchable corpus\ncorpus = []\nfor item in infoboxes:\n    title = item.get(\"title\", \"\")\n    infobox_text = item.get(\"infobox\", \"\")\n    if title and infobox_text:\n        corpus.append(f\"{title}: {infobox_text}\")\n\n# Initialize the retriever model\nretriever = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\ncorpus_embeddings = retriever.encode(corpus, convert_to_tensor=True)\n\n# Function to retrieve top-k context\ndef retrieve_context(query, top_k):\n    query_embedding = retriever.encode(query, convert_to_tensor=True)\n    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n    return [corpus[hit['corpus_id']] for hit in hits[0]]\n\n# Initialize the generative model\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n\n# Function to generate answer using the retrieved context\ndef generate_answer(query, top_k):\n    contexts = retrieve_context(query, top_k=top_k)\n    \n    # Clean and format the contexts\n    contexts_cleaned = []\n    for context in contexts:\n        context_cleaned = context.replace(\"[[\", \"\").replace(\"]]\", \"\").replace(\"'''\", \"\").replace(\"|\", \": \")\n        context_cleaned = context_cleaned.replace(\"\\n\", \" \").replace(\"  \", \" \")\n        contexts_cleaned.append(context_cleaned)\n    \n    combined_context = \" \".join(contexts_cleaned)\n    \n    # Ensure combined context does not exceed max length\n    max_length = 512\n    combined_tokens = tokenizer.encode(combined_context)\n    if len(combined_tokens) > max_length:\n        combined_tokens = combined_tokens[:max_length]\n        combined_context = tokenizer.decode(combined_tokens, skip_special_tokens=True)\n    \n    input_text = f\"Context: {combined_context}\\n\\nQuestion: {query}\\n\\nAnswer (please format as a timeline):\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n    outputs = model.generate(inputs.input_ids, max_length=200)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\n# Function to run experiments with different top_k values\ndef run_experiments(query):\n    for top_k in [3, 5, 7, 10]:\n        print(f\"Running experiment with top_k={top_k}\")\n        answer = generate_answer(query, top_k)\n        print(f\"Answer with top_k={top_k}:\\n{answer}\\n\")\n\n# Test the system with the sample question\nquery = \"What were the major historical periods of Nauru and their respective timelines?\"\nrun_experiments(query)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T11:10:13.898364Z","iopub.execute_input":"2024-05-26T11:10:13.898765Z","iopub.status.idle":"2024-05-26T11:13:47.275231Z","shell.execute_reply.started":"2024-05-26T11:10:13.898735Z","shell.execute_reply":"2024-05-26T11:13:47.274188Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6cd7c6ce1dc4d57b93ed65777071a13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094b6334494e44a99825aeae3a21fdda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2bd15b4ae049baa52e4f48430365d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6c5b6d16f4d45c48a2d42616810775a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abed16b402404692a93ed2e4a71d392f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d7a48bc8ef4232a5e9928096d308b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8109e101061460b8b09d90b65cc2255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"326f6878a0f446f8b6679562f3dcc80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97ced20a3f4453cb5e723d0ed67081b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f5e662924a34b638fda94575cf7d53c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a11c822f3dba4da3b5293e9540f5a047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4f50be65db4844b9e4bcb6d4f91f28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7408a8a302ed40d482a8fd2e39b31c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd17a958310490d8a67c8731ce9f7ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c780c21f51d44101afde07b559e267b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97aa9c051f84316a4ebcfe4660a8c5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47eb61118db74fd19d671d607d213197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03555cfae3544d0e80b9fbe37debe0a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c76f3764a68d434a88757a154392fe13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9c14b28f52431c9849b29ae7c9b50a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805cc04d13f74570a309af89a2289d7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcaa701ce7424cea8cc3800a9bf6cf64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111f4d0d7b8c47dc8a4aa9e894ee27c6"}},"metadata":{}},{"name":"stdout","text":"Running experiment with top_k=3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4660a621a134cd7a8c5ffea27cc16ee"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (947 > 512). Running this sequence through the model will result in indexing errors\n2024-05-26 11:11:36.319572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-26 11:11:36.319670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-26 11:11:36.442220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Answer with top_k=3:\nPre-history : data2 = until 1888 : German Empire: German Rule : data3 = 1888–1919 : Australia trust : data4 = 1920–1967 : Japanese occupation of Nauru: Japanese Rule : data5 = 1942–45 : Republic : data6 = 1968–present\n\nRunning experiment with top_k=5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3fa6e956257410a9b3106aa38c0f2fb"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=5:\nPre-history : data2 = until 1888 : German Empire: German Rule : data3 = 1888–1919 : Australia trust : data4 = 1920–1967 : Japanese occupation of Nauru: Japanese Rule : data5 = 1942–45 : Republic : data6 = 1968–present\n\nRunning experiment with top_k=7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73e1e34485924863bef91ee9f3900738"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=7:\nPre-history : data2 = until 1888 : German Empire: German Rule : data3 = 1888–1919 : Australia trust : data4 = 1920–1967 : Japanese occupation of Nauru: Japanese Rule : data5 = 1942–45 : Republic : data6 = 1968–present\n\nRunning experiment with top_k=10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28f1129cba4845a4811bd5f84fb1e916"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=10:\nPre-history : data2 = until 1888 : German Empire: German Rule : data3 = 1888–1919 : Australia trust : data4 = 1920–1967 : Japanese occupation of Nauru: Japanese Rule : data5 = 1942–45 : Republic : data6 = 1968–present\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Flan-T5-large + tas-b + evaluation?**","metadata":{}},{"cell_type":"code","source":"import json\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# Load infoboxes\nwith open('/kaggle/input/infoboxes-v1/extracted_infoboxes.json') as f:\n    infoboxes = json.load(f)\n\n# Extract relevant text from infoboxes and create a searchable corpus\ncorpus = []\nfor item in infoboxes:\n    title = item.get(\"title\", \"\")\n    infobox_text = item.get(\"infobox\", \"\")\n    if title and infobox_text:\n        corpus.append(f\"{title}: {infobox_text}\")\n\n# Initialize the retriever model\nretriever = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\ncorpus_embeddings = retriever.encode(corpus, convert_to_tensor=True)\n\n# Function to retrieve top-k context\ndef retrieve_context(query, top_k):\n    query_embedding = retriever.encode(query, convert_to_tensor=True)\n    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n    return [corpus[hit['corpus_id']] for hit in hits[0]]\n\n# Initialize the generative model\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n\n# Function to generate answer using the retrieved context\ndef generate_answer(query, top_k):\n    contexts = retrieve_context(query, top_k=top_k)\n    \n    # Clean and format the contexts\n    contexts_cleaned = []\n    for context in contexts:\n        context_cleaned = context.replace(\"[[\", \"\").replace(\"]]\", \"\").replace(\"'''\", \"\").replace(\"|\", \": \")\n        context_cleaned = context_cleaned.replace(\"\\n\", \" \").replace(\"  \", \" \")\n        contexts_cleaned.append(context_cleaned)\n    \n    combined_context = \" \".join(contexts_cleaned)\n    \n    # Ensure combined context does not exceed max length\n    max_length = 512\n    combined_tokens = tokenizer.encode(combined_context)\n    if len(combined_tokens) > max_length:\n        combined_tokens = combined_tokens[:max_length]\n        combined_context = tokenizer.decode(combined_tokens, skip_special_tokens=True)\n    \n    input_text = f\"Context: {combined_context}\\n\\nQuestion: {query}\\n\\nAnswer (please format as a timeline):\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n    outputs = model.generate(inputs.input_ids, max_length=200)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\n# Function to run experiments with different top_k values\ndef run_experiments(query):\n    results = {}\n    for top_k in [3, 5, 7, 10]:\n        print(f\"Running experiment with top_k={top_k}\")\n        answer = generate_answer(query, top_k)\n        results[top_k] = answer\n        print(f\"Answer with top_k={top_k}:\\n{answer}\\n\")\n    return results\n\n# Function to evaluate performance\ndef evaluate_performance(results, ground_truth):\n    # Implement evaluation metrics: EM, F1, TimeMetric, Completeness\n    evaluation_results = {}\n    for top_k, answer in results.items():\n        # Placeholder for actual evaluation logic\n        evaluation_results[top_k] = {\"EM\": 0, \"F1\": 0, \"TimeMetric\": 0, \"Completeness\": 0}\n    return evaluation_results\n\n# Test the system with the sample question\nquery = \"What were the major historical periods of Nauru and their respective timelines?\"\nresults = run_experiments(query)\nground_truth = \"Provide the actual ground truth here\"  # Placeholder\nevaluation_results = evaluate_performance(results, ground_truth)\nprint(f\"Evaluation Results: {evaluation_results}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T11:27:23.575581Z","iopub.execute_input":"2024-05-26T11:27:23.576059Z","iopub.status.idle":"2024-05-26T11:28:36.770959Z","shell.execute_reply.started":"2024-05-26T11:27:23.576018Z","shell.execute_reply":"2024-05-26T11:28:36.769878Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc336f95825473d9fb01bd6e84ed1b0"}},"metadata":{}},{"name":"stdout","text":"Running experiment with top_k=3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af8bd05e8864908a0ed5499fcbe33c8"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (947 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Answer with top_k=3:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\nRunning experiment with top_k=5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eac9f07c1004da5ac34485f0e42ce87"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=5:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\nRunning experiment with top_k=7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b518fb5b7874551bdb1852285aad278"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=7:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\nRunning experiment with top_k=10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23eecc4267ef491296e7f22c761c35d7"}},"metadata":{}},{"name":"stdout","text":"Answer with top_k=10:\nPre-history : until 1888 : German Empire : German Rule : 1888–1919 : Australia trust : 1920–1967 : Japanese occupation of Nauru : Japanese Rule : 1942–45 : Republic : 1968–present : Major Events : Phosphate originally found : 1900 : Collapse of phosphate industry : 2002 The Goodies : Infobox : name = : title : The Goodies : image = File:TheGoodies.jpg : 240px : caption\n\nEvaluation Results: {3: {'EM': 0, 'F1': 0, 'TimeMetric': 0, 'Completeness': 0}, 5: {'EM': 0, 'F1': 0, 'TimeMetric': 0, 'Completeness': 0}, 7: {'EM': 0, 'F1': 0, 'TimeMetric': 0, 'Completeness': 0}, 10: {'EM': 0, 'F1': 0, 'TimeMetric': 0, 'Completeness': 0}}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}